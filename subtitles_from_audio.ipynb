{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOaJ3wGqVLVIwtcF4HGjztq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theaidran/subtitles_from_audio/blob/main/subtitles_from_audio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "hGyi3911u5om",
        "outputId": "d98eab92-09c9-4dce-d6b9-0b7a9f433065"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/m-bain/whisperx.git /tmp/pip-req-build-y89fbyrv\n",
            "Requirement already satisfied: ffmpeg-python in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python) (1.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Detected duration: 66.38 seconds for /content/myfile.wav\n",
            "Detected file type: audio/x-wav\n",
            "Audio/Media duration: 66.38\n",
            "Loading WhisperX large-v2 model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.4.1+cu121. Bad things might happen unless you revert torch to 1.x.\n",
            "Transcribing media with WhisperX...\n",
            "Detected language: en (1.00) in first 30s of audio...\n",
            "Detected language: en\n",
            "Subtitles generated and saved as: myfile.srt\n",
            "Creating video with black background and audio...\n",
            "Embedding subtitles into the video with custom font size...\n",
            "Final video with subtitles saved as: myfile_with_subtitles.mp4\n",
            "Downloading the generated SRT file: myfile.srt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d6792b98-5df2-48b7-87ea-41e467c1f946\", \"myfile.srt\", 2120)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install git+https://github.com/m-bain/whisperx.git > /dev/null\n",
        "!pip install ffmpeg-python\n",
        "!pip install torch  # Install PyTorch if not already available\n",
        "import mimetypes\n",
        "import subprocess\n",
        "import json\n",
        "import os\n",
        "from google.colab import files  # Import to trigger file downloads in Colab\n",
        "\n",
        "# Colab form inputs\n",
        "#@markdown Upload your audio file (wav, mp3, mp4, mkv) to the Colab default directory (menu on the left).\n",
        "#@markdown Insert your file name:\n",
        "media_file = \"/content/myfile.wav\" # @param {\"type\":\"string\",\"placeholder\":\"your file \"}\n",
        "\n",
        "#@markdown myfile.srt file will be automatically downloaded when ready\n",
        "\n",
        "#@markdown ---\n",
        "embed_subtitles_into_mp4_video = False  # @param {\"type\":\"boolean\"}\n",
        "use_black_background = True  #@param {type:\"boolean\"}\n",
        "optional_image_file = \"\"  # @param {\"type\":\"string\",\"placeholder\":\"jpeg, png, tif, etc\"}\n",
        "font_size = \"default\"  # @param [\"default\",\"12\",\"14\",\"16\",\"18\",\"20\",\"22\",\"24\",\"26\",\"28\",\"30\",\"32\",\"34\",\"36\",\"None\"] {\"allow-input\":true}\n",
        "output_resolution = \"1280x720\"  # @param [\"3840x2160\",\"1920x1080\",\"1280x720\",\"854x480\"]\n",
        "\n",
        "#@markdown The embedding process time depends on file formats, for the input '.wav' and the output resolution of 1280x720; it takes about half the length of the audio file.\n",
        "\n",
        "#@markdown File myfile_with_subtitles.mp4 will be in /content when ready\n",
        "# Import required libraries\n",
        "import whisperx\n",
        "import ffmpeg  # Import the ffmpeg-python binding\n",
        "import torch  # Import torch for device detection\n",
        "import wave  # To calculate the duration of the .wav file\n",
        "\n",
        "# Helper function to convert time in seconds to SRT time format\n",
        "def convert_time_srt_format(seconds):\n",
        "    milliseconds = int((seconds % 1) * 1000)\n",
        "    seconds = int(seconds)\n",
        "    hours = seconds // 3600\n",
        "    minutes = (seconds % 3600) // 60\n",
        "    seconds = seconds % 60\n",
        "    return f\"{hours:02}:{minutes:02}:{seconds:02},{milliseconds:03}\"\n",
        "\n",
        "# Function to detect the file type\n",
        "def detect_file_type(file_path):\n",
        "    mime_type, _ = mimetypes.guess_type(file_path)\n",
        "    return mime_type\n",
        "\n",
        "# Function to get media info using ffprobe\n",
        "def get_media_info(file_path):\n",
        "    result = subprocess.run(\n",
        "        ['ffprobe', '-v', 'error', '-show_entries', 'format=format_name', '-of', 'json', file_path],\n",
        "        stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
        "    )\n",
        "    return json.loads(result.stdout)\n",
        "\n",
        "# Function to get audio duration from a .wav or other media files\n",
        "def get_audio_duration(file_path):\n",
        "    media_info = get_media_info(file_path)\n",
        "    format_name = media_info['format']['format_name']\n",
        "\n",
        "    if 'wav' in format_name:\n",
        "        with wave.open(file_path, \"r\") as audio:\n",
        "            frames = audio.getnframes()\n",
        "            rate = audio.getframerate()\n",
        "            duration = frames / float(rate)\n",
        "    else:\n",
        "        # For other media formats like mp3, mp4, etc.\n",
        "        result = subprocess.run(\n",
        "            ['ffprobe', '-v', 'error', '-show_entries', 'format=duration', '-of', 'default=noprint_wrappers=1:nokey=1', file_path],\n",
        "            stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
        "        )\n",
        "        duration = float(result.stdout.strip())\n",
        "\n",
        "    print(f\"Detected duration: {duration} seconds for {file_path}\")\n",
        "    return duration\n",
        "\n",
        "# Helper function to run FFmpeg and capture stderr output for debugging\n",
        "def run_ffmpeg_with_error_capture(ffmpeg_command):\n",
        "    try:\n",
        "        # Capture both stdout and stderr\n",
        "        ffmpeg_command.run(overwrite_output=True, capture_stdout=True, capture_stderr=True)\n",
        "    except ffmpeg.Error as e:\n",
        "        # Capture and print the stderr output to understand the FFmpeg error\n",
        "        print(f\"FFmpeg Error: {e.stderr.decode('utf-8') if e.stderr else 'No stderr output available.'}\")\n",
        "        raise e\n",
        "\n",
        "# Step 1: Validate form inputs (media file and optional image)\n",
        "if not media_file:\n",
        "    raise ValueError(\"Please provide a media file using the form.\")\n",
        "\n",
        "# Extract the base name of the media file (without extension)\n",
        "base_name = os.path.splitext(os.path.basename(media_file))[0]\n",
        "\n",
        "# Step 2: Detect file type and duration\n",
        "media_type = detect_file_type(media_file)\n",
        "audio_duration = get_audio_duration(media_file)\n",
        "\n",
        "print(f\"Detected file type: {media_type}\")\n",
        "print(f\"Audio/Media duration: {audio_duration}\")\n",
        "\n",
        "# Step 3: Load WhisperX large-v2 model and transcribe the audio\n",
        "print(\"Loading WhisperX large-v2 model...\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Automatically set to GPU if available, otherwise use CPU\n",
        "model = whisperx.load_model(\"large-v2\", device=device)\n",
        "\n",
        "print(\"Transcribing media with WhisperX...\")\n",
        "result = model.transcribe(media_file)\n",
        "\n",
        "# Detect language from the transcription\n",
        "language_code = result['language']\n",
        "print(f\"Detected language: {language_code}\")\n",
        "\n",
        "# Align the transcription with timestamps\n",
        "model_a, metadata = whisperx.load_align_model(language_code=language_code, device=device)\n",
        "result_aligned = whisperx.align(result[\"segments\"], model_a, metadata, media_file, device=device)\n",
        "\n",
        "# Step 4: Save the transcription as a .srt file with the same base name as media_file\n",
        "srt_filename = f\"{base_name}.srt\"\n",
        "with open(srt_filename, \"w\") as srt_file:\n",
        "    for i, segment in enumerate(result_aligned[\"segments\"]):\n",
        "        start_time = segment[\"start\"]\n",
        "        end_time = segment[\"end\"]\n",
        "        text = segment[\"text\"]\n",
        "\n",
        "        # Convert start and end time to SRT format (hh:mm:ss,ms)\n",
        "        start_srt = convert_time_srt_format(start_time)\n",
        "        end_srt = convert_time_srt_format(end_time)\n",
        "\n",
        "        srt_file.write(f\"{i + 1}\\n\")\n",
        "        srt_file.write(f\"{start_srt} --> {end_srt}\\n\")\n",
        "        srt_file.write(f\"{text.strip()}\\n\\n\")\n",
        "\n",
        "print(f\"Subtitles generated and saved as: {srt_filename}\")\n",
        "\n",
        "# Properly escape the SRT file path\n",
        "srt_filename_escaped = srt_filename.replace(\":\", \"\\\\:\").replace(\" \", \"\\\\ \")\n",
        "\n",
        "# Step 5: Handle background (either image, black background, or no background)\n",
        "output_video = f\"{base_name}_output.mp4\"\n",
        "\n",
        "if embed_subtitles_into_mp4_video:\n",
        "    if optional_image_file:\n",
        "        print(\"Creating video with image background and audio...\")\n",
        "\n",
        "        # Use the provided image as the background and resize it to ensure height is divisible by 2\n",
        "        image_input = (\n",
        "            ffmpeg.input(optional_image_file, loop=1)\n",
        "            .filter('scale', 'trunc(iw/2)*2', 'trunc(ih/2)*2')  # Ensure width and height are divisible by 2\n",
        "        )\n",
        "        audio_input = ffmpeg.input(media_file)  # Audio input\n",
        "\n",
        "        # Combine image and audio, with error capture\n",
        "        run_ffmpeg_with_error_capture(\n",
        "            ffmpeg.output(image_input, audio_input, output_video, vcodec='libx264', acodec='aac', pix_fmt='yuv420p', shortest=None, strict='experimental')\n",
        "        )\n",
        "\n",
        "    elif use_black_background:\n",
        "        print(\"Creating video with black background and audio...\")\n",
        "\n",
        "        # Create a black background video with the specified resolution\n",
        "        run_ffmpeg_with_error_capture(\n",
        "            ffmpeg.input(f'color=c=black:s={output_resolution}', f='lavfi').output('black_background.mp4', t=1, vcodec='libx264')\n",
        "        )\n",
        "\n",
        "        # Define separate inputs for background and audio\n",
        "        background_input = ffmpeg.input('black_background.mp4', stream_loop=-1)  # Loop the black background\n",
        "        audio_input = ffmpeg.input(media_file)  # Audio input\n",
        "\n",
        "        # Combine video and audio, with error capture\n",
        "        run_ffmpeg_with_error_capture(\n",
        "            ffmpeg.output(background_input, audio_input, output_video, vcodec='libx264', acodec='aac', pix_fmt='yuv420p', shortest=None, strict='experimental')\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        # If no background is needed, just use the original media file\n",
        "        print(\"Using the original video file as input...\")\n",
        "        output_video = media_file\n",
        "\n",
        "# Step 6: Add subtitles to the video if \"embed_subtitles_into_mp4_video\" is selected\n",
        "output_video_with_subtitles = f\"{base_name}_with_subtitles.mp4\"\n",
        "\n",
        "if embed_subtitles_into_mp4_video:\n",
        "    print(\"Embedding subtitles into the video with custom font size...\")\n",
        "\n",
        "    # Check if the user has selected a custom font size or default\n",
        "    if font_size != \"default\":\n",
        "        font_size_style = f\"FontSize={font_size}\"\n",
        "    else:\n",
        "        font_size_style = \"\"\n",
        "\n",
        "    # Apply subtitles to the final video with custom font size using force_style\n",
        "    run_ffmpeg_with_error_capture(\n",
        "        ffmpeg.input(output_video).output(\n",
        "            output_video_with_subtitles,\n",
        "            vcodec='libx264',\n",
        "            acodec='aac',\n",
        "            vf=f\"subtitles='{srt_filename_escaped}':force_style='{font_size_style}'\"\n",
        "        )\n",
        "    )\n",
        "else:\n",
        "    # If captions are not embedded, just rename the output video without subtitles\n",
        "    print(\"Not embedding subtitles. Skipping subtitle embedding.\")\n",
        "    output_video_with_subtitles = output_video\n",
        "\n",
        "print(f\"Final video with subtitles saved as: {output_video_with_subtitles}\")\n",
        "\n",
        "# Step 7: Automatically trigger download of the SRT file\n",
        "print(f\"Downloading the generated SRT file: {srt_filename}\")\n",
        "files.download(srt_filename)  # Automatically download the SRT file\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dYZbXS_zyPJH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}